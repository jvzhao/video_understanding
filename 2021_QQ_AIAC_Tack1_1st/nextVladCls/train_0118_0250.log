02:50:50 [INFO] Start
02:50:50 [INFO] ==================================================================
02:50:50 [INFO] Config - finetune:
# Finetune config
# Pretrain model path
PRETRAIN_PATH='./model_pretrain_1.pth'
# Training params
NUM_FOLDS = 5
SEED = 2021
BATCH_SIZE = 32
NUM_EPOCHS = 10
WARMUP_RATIO = 0.06
REINIT_LAYER = 0
WEIGHT_DECAY = 0.01
LR = {'others':5e-5, 'nextvlad':5e-5, 'roberta':1e-5}
LR_LAYER_DECAY = 0.975
02:50:50 [INFO] ==================================================================
02:50:50 [INFO] Config - model:
MODEL_TYPE = 'uni'#'all', 'cross', 'frame', 'bi', 'uni'

MODEL_CONFIG = {
    'INPUT_SIZE': 1792,
    'HIDDEN_SIZE': 256,
    'NUM_CLASSES': 10000,
    'FEATURE_SIZE': 1536,
    'OUTPUT_SIZE': 1024,
    'EXPANSION_SIZE': 2,
    'CLUSTER_SIZE': 64,
    'NUM_GROUPS': 8,
    'DROPOUT_PROB': 0.2,
}

BERT_CFG_DICT = {}
BERT_CFG_DICT['uni'] = {
    'hidden_size':768,
    'num_hidden_layers':6,
    'num_attention_heads':12,
    'intermediate_size':3072,
    'hidden_dropout_prob':0.0,
    'attention_probs_dropout_prob':0.0
}
02:50:50 [INFO] ==================================================================
02:50:50 [INFO] Config - data:
DATA_PATH = '../input/data'
BERT_PATH = '../input/pretrain-model/roberta-wwm-large'

DESC = {
    'tag_id':"int",
    'id': 'byte',
    'category_id': 'int',
    'title': 'byte',
    'asr_text': 'byte',
    'frame_feature': 'bytes'
}

DESC_NOTAG = {
    'id': 'byte',
    'title': 'byte',
    'asr_text': 'byte',
    'frame_feature': 'bytes'
}
02:50:50 [INFO] Model_type = uni
02:51:23 [INFO] Load pair dataset finish
02:51:39 [INFO] Load test dataset finish
02:51:39 [INFO] ==================================================================
02:51:39 [INFO] Fold=1/5 seed=2021 train_len=65373 val_len=2526
02:51:39 [INFO] Total train steps=20429, warmup steps=1225
