03:49:05 [INFO] Start
03:49:05 [INFO] ==================================================================
03:49:05 [INFO] Config - pretrain:
# Pretrain file num
PRETRAIN_FILE_NUM = 20
LOAD_DATA_TYPE = 'mem'#'fluid'
# Training params
NUM_FOLDS = 1
SEED = 2021
BATCH_SIZE = 128
NUM_EPOCHS = 40
WARMUP_RATIO = 0.06
REINIT_LAYER = 0
WEIGHT_DECAY = 0.01
LR = {'others':5e-4, 'roberta':5e-5, 'newfc_videoreg':5e-4}
LR_LAYER_DECAY = 1.0
PRETRAIN_TASK = ['tag', 'mlm', 'mfm']
03:49:05 [INFO] ==================================================================
03:49:05 [INFO] Config - model:
MODEL_TYPE = 'uni'#'all', 'cross', 'frame', 'bi', 'uni'

MODEL_CONFIG = {
    'INPUT_SIZE': 1792,
    'HIDDEN_SIZE': 256,
    'NUM_CLASSES': 10000,
    'FEATURE_SIZE': 1536,
    'OUTPUT_SIZE': 1024,
    'EXPANSION_SIZE': 2,
    'CLUSTER_SIZE': 64,
    'NUM_GROUPS': 8,
    'DROPOUT_PROB': 0.2,
}

BERT_CFG_DICT = {}
BERT_CFG_DICT['uni'] = {
    'hidden_size':768,
    'num_hidden_layers':6,
    'num_attention_heads':12,
    'intermediate_size':3072,
    'hidden_dropout_prob':0.0,
    'attention_probs_dropout_prob':0.0
}
03:49:05 [INFO] ==================================================================
03:49:05 [INFO] Config - data:
DATA_PATH = '../input/data'
BERT_PATH = '../input/pretrain-model/roberta-wwm-large'

DESC = {
    'tag_id':"int",
    'id': 'byte',
    'category_id': 'int',
    'title': 'byte',
    'asr_text': 'byte',
    'frame_feature': 'bytes'
}

DESC_NOTAG = {
    'id': 'byte',
    'title': 'byte',
    'asr_text': 'byte',
    'frame_feature': 'bytes'
}
03:49:05 [INFO] Model_type = uni
03:49:05 [INFO] ==================================================================
03:49:05 [INFO] Fold=1/1 seed=2021
03:49:05 [INFO] Load data into memory
03:49:51 [INFO] Dataset used memory = 6.6GB
03:49:51 [INFO] Total train steps=19879, warmup steps=1192
